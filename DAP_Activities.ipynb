{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failte Ireland API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Data.Gov Activities API and print response\n",
    "response = requests.get(\"https://failteireland.portal.azure-api.net/docs/services/opendata-api-v1/operations/activities-get\")\n",
    "print(response.status_code)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review if data we received back from the API is JSON:\n",
    "url = \"https://failteireland.portal.azure-api.net/docs/services/opendata-api-v1/operations/activities-get\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if 'json' in r.headers.get('Content-Type'):\n",
    "    js = r.json()\n",
    "else:\n",
    "    print('Response content is not in JSON format.')\n",
    "    js = 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read non JSON API data using URL Parse \n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "headers = {}\n",
    "\n",
    "params = urllib.parse.urlencode({})\n",
    "jsondata = []\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('failteireland.azure-api.net')\n",
    "    conn.request(\"GET\", \"/opendata-api/v1/activities?%s\" % params, \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "#    for r in data:\n",
    "#        print(r)\n",
    "    jsond = json.loads(data)\n",
    "    jsondata.append(jsond)\n",
    "    print(jsondata)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reattempt to read API Data with dataframe\n",
    "\n",
    "import urllib.request as request\n",
    "import pandas as pd\n",
    "import json\n",
    "Name, Telephone = [],[]\n",
    "jsondata = []\n",
    "\n",
    "with request.urlopen('https://failteireland.azure-api.net/opendata-api/v1/activities') as response:\n",
    "    source = response.read()\n",
    "for line in source:\n",
    "    try:\n",
    "        jsond = json.loads(source)\n",
    "        jsond2 = jsond['results']\n",
    "        for i in jsondata:\n",
    "            Name.append(i['name'])\n",
    "            Telephone.append(i['telephone'])\n",
    "        df = pd.DataFrame([Name,Telephone]).T\n",
    "#        jsondata.append(jsond)\n",
    "    except Exception as e:\n",
    "        print(e)            \n",
    "#    jsondata = json.loads(source)\n",
    "#    jsond = json.loads(source)\n",
    "#    jsondata.append(jsond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panda Flatten Script\n",
    "from itertools import chain, starmap\n",
    "\n",
    "def flatten_json_iterative_solution(dictionary):\n",
    "    \"\"\"Flatten a nested json file\"\"\"\n",
    "\n",
    "    def unpack(parent_key, parent_value):\n",
    "        \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "        if isinstance(parent_value, dict):\n",
    "            for key, value in parent_value.items():\n",
    "                temp1 = parent_key + '_' + key\n",
    "                yield temp1, value\n",
    "        elif isinstance(parent_value, list):\n",
    "            i = 0 \n",
    "            for value in parent_value:\n",
    "                temp2 = parent_key + '_'+str(i) \n",
    "                i += 1\n",
    "                yield temp2, value\n",
    "        else:\n",
    "            yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "    # Keep iterating until the termination condition is satisfied\n",
    "    while True:\n",
    "        # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "        dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "        # Terminate condition: not any value in the json file is dictionary or list\n",
    "        if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "           not any(isinstance(value, list) for value in dictionary.values()):\n",
    "            break\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "### Did not need to use this in the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for API\n",
    "activities = pd.DataFrame()\n",
    "activities['name'] = list(map(lambda jsond: jsond['name'], jsondata))\n",
    "activities['url'] = list(map(lambda jsond: jsond['url'], jsondata))\n",
    "activities['AddressRegion'] = list(map(lambda jsond: jsond['AddressRegion'], jsondata))\n",
    "activities['AddressLocality'] = list(map(lambda jsond: jsond['AddressLocality'], jsondata))\n",
    "activities.head()\n",
    "\n",
    "### Did not need to use this in the end as I exported to CSV as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to convert JSON file to CSV\n",
    "  \n",
    "import json\n",
    "import csv\n",
    "  \n",
    "# Opening JSON file and loading the data\n",
    "# into the variable data\n",
    "with open('activities.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "activities_data = data['results']\n",
    "  \n",
    "# now we will open a file for writing\n",
    "data_file = open('activities_file.csv', 'w')\n",
    "  \n",
    "# create the csv writer object\n",
    "csv_writer = csv.writer(data_file)\n",
    "  \n",
    "# Counter variable used for writing \n",
    "# headers to the CSV file\n",
    "count = 0\n",
    "  \n",
    "for results in activities_data:\n",
    "    if count == 0:\n",
    "  \n",
    "        # Writing headers of CSV file\n",
    "        header = results.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    "  \n",
    "    # Writing data of CSV file\n",
    "    csv_writer.writerow(results.values())\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading activities_file dataset \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "act_data = pd.read_csv(r\"C:\\Users\\korpe\\Desktop\\Work\\Activities\\Activities_Failte.csv\")\n",
    "#print(act_data)\n",
    "\n",
    "#Dealing with headers\n",
    "print(act_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the csv into a dataFrame\n",
    "activitiesfile = \"C://Users/korpe/Desktop/Work/Activities/Activities_Failte.csv\"\n",
    "activities_df = pd.read_csv(activitiesfile)\n",
    "print(activities_df)\n",
    "\n",
    "# View the head of the DataFrame\n",
    "print(activities_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas, create an array from the dataframe\n",
    "activities_df_array = activities_df.values\n",
    "print(activities_df.head())\n",
    "\n",
    "# Check the datatype of activities_df_array\n",
    "print(type(activities_df_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information of 'activities_df'\n",
    "activities_df.info()\n",
    "\n",
    "# Before handling missing values in dataframe, will import to Mongodb to save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving pandas dataframe into json before sending to MongoDB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "  \n",
    "dataFrame = activities_df\n",
    "dataFrame.to_json(r'C:\\Users\\korpe\\Desktop\\Work\\Activities\\Activities_Failte.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json into mongdb\n",
    "import json\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient \n",
    "  \n",
    "# Making Connection\n",
    "myclient = pymongo.MongoClient('192.168.56.30', 27017)\n",
    "   \n",
    "# database \n",
    "db = client.activitiesdb\n",
    "   \n",
    "# Created or Switched to collection names\n",
    "Collection = db.activitiescollection\n",
    "  \n",
    "# Loading or Opening the dataframe\n",
    "with open('Activities_Failte.json') as file:\n",
    "    file_data = json.load(file)\n",
    "      \n",
    "# Inserting the loaded data in the Collection if JSON contains data more than one entry insert_many is used else inser_one is used\n",
    "if isinstance(file_data, list):\n",
    "    Collection.insert_many(file_data)  \n",
    "else:\n",
    "    Collection.insert_one(file_data) \n",
    "    \n",
    "# Print Output of activities db to confirm Mongodb load\n",
    "for m in Collection.find({}):\n",
    " print(m)\n",
    "\n",
    "# Same procedure used to upload into a new Collection after data wrangling step below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy, create an array from the dataframe\n",
    "activities_df_array = activities_df.values\n",
    "print(activities_df.head())\n",
    "\n",
    "# Check the datatype of activities_df_array\n",
    "print(type(activities_df_array))\n",
    "\n",
    "# Print information of 'activities_df'\n",
    "activities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Url, Telephone, Longitude, Latitude, AddressRegion, AddressLocality, AddressCountry, Tags]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# There are a total of 5413 rows and 9 columns in the dataframe.\n",
    "\n",
    "# Before handling missing data, create copy of activities_df\n",
    "activities_df_missing = activities_df.copy()\n",
    "df = activities_df_missing\n",
    "#activities_df.head()\n",
    "\n",
    "# For the merge-join of weather data for the project, the critical data is the Address region. \n",
    "# We will need to replace NaN in the AddressRegion column with correct region data.\n",
    "\n",
    "# Identify missing values in the AddressRegion column\n",
    "print (df[df['AddressRegion'].isna()])\n",
    "\n",
    "# Select all rows with NaN under AddressRegion DataFrame column\n",
    "nan_values = df[df['AddressRegion'].isna()]\n",
    "print (nan_values)\n",
    "\n",
    "# Confirm all empty fields using iloc function beginning with the First\n",
    "\n",
    "df.iloc[379][5]\n",
    "\n",
    "# Replace NaN values with a Region label based on the locality and URL\n",
    "\n",
    "df.iat[379,5] = 'Cork'\n",
    "df.iat[2186,5] = 'Cork'\n",
    "df.iat[2191,5] = 'Donegal'\n",
    "df.iat[2298,5] = 'Cork'\n",
    "df.iat[2358,5] = 'Offaly'\n",
    "df.iat[2908,5] = 'Wicklow'\n",
    "df.iat[3167,5] = 'Mayo'\n",
    "df.iat[3682,5] = 'Cork'\n",
    "df.iat[3959,5] = 'Galway'\n",
    "df.iat[3993,5] = 'Cork'\n",
    "\n",
    "# Confirm No NaN are present in the AddressRegion Column\n",
    "\n",
    "nan_values = df[df['AddressRegion'].isna()]\n",
    "print (nan_values)\n",
    "\n",
    "# The remaning NaN are acceptable as is some cases there are no fields present for these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellanous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for Pandas Dataframe\n",
    "def flatten_dict(d):\n",
    "    \"\"\" Returns list of lists from given dictionary \"\"\"\n",
    "    l = []\n",
    "    for k, v in sorted(d.items()):\n",
    "        if isinstance(v, dict):\n",
    "            flatten_v = flatten_dict(v)\n",
    "            for my_l in reversed(flatten_v):\n",
    "                my_l.insert(0, k)\n",
    "\n",
    "            l.extend(flatten_v)\n",
    "\n",
    "        elif isinstance(v, list):\n",
    "            for l_val in v:\n",
    "                l.append([k, l_val])\n",
    "\n",
    "        else:\n",
    "            l.append([k, v])\n",
    "\n",
    "    return l\n",
    "\n",
    "df = pd.DataFrame(flatten_dict(jsond))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(flatten_dict(jsond))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(jsondata)\n",
    "json_string = json.dumps(jsondata)\n",
    "print(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
